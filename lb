Cài đặt hệ thống HA-LB sử dụng Corosync, Pacemaker và Ldirectord
I. MỤC ĐÍCH SỬ DỤNG
Mục đích của công nghệ HA-LB là dùng để xây dựng một hệ thống:
Tính sẵn sàng cao: toàn bộ hệ thống không có điểm chết (bất cứ thành phần nào cũng luôn đảm bảo tính dự phòng)
Tính cân bằng tải: Chia đều tải cho nhiều tài nguyên cùng tính toán, xử lý. Có thể dễ dàng thêm bớt các tài nguyên tùy theo nhu cầu
Ngoài ra, công nghệ HA-LB còn giúp xây dựng hệ thống server hoàn toàn trong suốt với client. Các client chỉ biết giao tiếp với hệ thống Server thông qua một IP duy nhất (VIP)
Hiện nay, để xây dựng hệ thống HA-LB thông thường có hai loại nhóm giải pháp chính là : (Corosync + Pacemaker + Ldirectord) và (HAProxy + Keepalived). Giải pháp sử dụng (HAProxy + Keepalived) có cấu hình đơn giản hơn nhưng không linh hoạt bằng giải pháp sử dụng (Corosync + Pacemaker + Ldirectord).
Trong tài liệu này mô tả công nghệ HA-LB xây dựng trên các phần mềm mã nguồn mở Corosync, Pacemaker, Ldirectord.
II. PHẠM VI, TIÊU CHÍ SỬ DỤNG
Phạm vi và tiêu chí sử dụng công nghệ HA-LB như sau:
Môi trường hệ điều hành Linux (các distribution sau: Centos, Fedora, Ubuntu, Debian)
Mô hình cân bằng tải này không đảm bảo được tính chất chia sẻ tài nguyên chung (ví dụ chia sẻ session trong WebServer,…), do vậy chỉ sử dụng mô hình này khi các Node đảm bảo một trong các tiêu chí sau :
Các dịch vụ hoạt động độc lập không giao tiếp, phụ thuộc với nhau. Ví dụ WebService
Các dịch vụ hoạt động có chia sẻ tài nguyên chung nhưng dịch vụ đó tự đảm bảo việc chia sẻ tài nguyên. Ví dụ như các hệ thống Database Clustering.
Dưới đây là một số ví dụ về các trường hợp có thể không dùng được mô hình HA-LB này:
Trường hợp 1 : Sử dụng LB cho WebServer. Khi đó, một User đăng nhập thành công và cấp session lưu trên WebServer1 (Node1). Request tiếp theo được cân bằng tải và xử lý trên WebServer2 (Node2). Do WebServer2 không chứa session của User nên nó gửi response về cho User trạng thái chưa đăng nhập. Để khắc phục tình trạng này có thể sử dụng các giải pháp đồng bộ Session giữa các WebServer. Giải pháp này nằm ngoài phạm vi của tài liệu này.
Trường hợp 2 : Sử dụng LB cho các Node xử lý nghiệp vụ theo các task trong DB. Khi đó, các Node tự động quét các task và thực thi. Tuy nhiên, có thể gặp trường hợp một task đồng thời được xử lý bởi nhiều Node dẫn đến sai về luồng nghiệp vụ. Để khắc phục tình trạng này cần đảm bảo mỗi task chỉ xử lý bởi duy nhất một Node bằng cơ chế transactional trong Database hoặc thiết kế luồng nghiệp vụ để tránh tình trạng đó.
III. MÔ HÌNH TỔNG QUAN
Giải pháp HA-LB bao gồm các phần mềm nguồn mở sau:
Corosync: (chi tiết: http://corosync.github.io/corosync ) cung cấp tính năng high availability cho các ứng dụng. Nó hỗ trợ quản lý, kiểm tra trạng thái hoạt động và có thể restart ứng dụng khi gặp lỗi.
Pacemaker: (chi tiết: http://clusterlabs.org ) thực hiện quản lý các resource.
Ldirectord: (chi tiết: http://horms.net/projects/ldirectord ) thực hiện cân bằng tải ở mức Layer 4.
Mô hình tổng quan:
Mô hình tổng quan
Hình trên là mô hình tổng quan hệ thống. Hệ thống hoạt động theo mô hình Active/Passive. Các thành phần chính như sau:
Tầng Hardware: Có hai node LB1 và LB2. Trong mô hình Active/Passive luôn có một node là Active và một node là Passive (dự phòng cho node Active). Ví dụ như hình trên LB1 là node Active, LB2 là node Passive.
Tầng Cluster Stack: Hai node hoạt động clustering với nhau thực hiện kiểm tra trạng thái (check heartbeat), quản lý các resource.
Tầng Service: Chứa các resource do Pacemaker quản lý. Các resource này luôn chạy trên node Active. Khi node Active gặp lỗi, node Passive sẽ tự động trở thành Active và chạy các Service để sẵn sàng cung cấp dịch vụ. Trong hệ thống HA-LB, hai dịch vụ được cung cấp là Virtual IP và Load Balancer. Service Virtual IP thực hiện gán một địa chỉ IP (VIP) cho một network interface ảo. Service Load Balacer thực hiện chạy phần mềm Ldirectord để cân bằng tải cho các dịch vụ.
IV. CÁC MÔ HÌNH TRIỂN KHAI HỆ THỐNG HA-LB
Có hai mô hình triển khai hệ thống HA-LB tùy thuộc vào nhu cầu sử dụng là mô hình NAT và mô hình ROUTE (phân loại dựa trên cách loadbalancer)
4.1 Mô hình NAT
Trong mô hình NAT, các connection được NAT từ LB xuống các Node dịch vụ. Mô hình triển khai như sau (lưu ý các IP trong mô hình này chỉ là ví dụ, có thể chỉnh sửa tùy theo thực tế):
Mô hình này gồm các thành phần sau:
Mô hình NAT
Hai node LB1, LB2 hoạt động theo mô hình Active/Passive cung cấp dịch vụ Virtual IP và Load Balancer. Hai node này đều có 2 network interface (một network interface cho kết nối external, một network interface cho kết nối nội bộ internal). LB1 và LB2 được cài đặt các phần mềm corosync, pacemaker và ldirectord. Corosync và pacemaker thực hiện HA trên hai node theo mô hình Active/Passive. Resource ldirectord chạy theo mô hình NAT. Resource Virtual IP cung cấp 2 VIP cho tương ứng hai dải mạng external và internal.
Các node dịch vụ chỉ cài đặt các phần mềm dịch vụ. Các node dịch vụ chỉ cần một network interface có dải mạng internal. Các node này có thể thêm, bớt tùy theo nhu cầu. Mỗi lần thêm bớt các node dịch vụ chỉ cần cấu hình ldirectord trên LB1 và LB2. Tất cả các node dịch vụ đều có default gateway là VIP2.
Luồng hoạt động của mô hình NAT như sau:
NAT Flow
Giả sử, hiện tại node LB1 đang là Active, node LB2 là Passive. Khi đó các dịch vụ Virtual IP và Ldirectord sẽ chạy trên node LB1.
(1) Từ một máy tính bên ngoài (ExIP) kết nối tới hệ thống theo IP là VIP1: 192.168.1.10. Trên LB1 nhận được các packet (ExIP -> VIP1)
(2) Dịch vụ Load balancer sẽ thực hiện cân bằng tải xuống các Node1, Node2, Node3. Do sử dụng mô hình NAT nên LB1 sẽ thực hiện sửa địa chỉ đích của các packet thành địa chỉ của node dịch vụ tương ứng. Giả sử, LB1 thực hiện LB xuống Node1. Khi đó trên Node1 nhận được các packet (ExIP -> Node1_IP)
(3) Node1 xử lý và gửi kết quả trả về là các packet (Node1_IP -> ExIP). Do ExIP là địa chỉ không cùng network với Node1_IP nên các packet này được gửi về gateway là VIP2 (tức LB1).
(4) Trên LB1, ldirectord dựa trên bảng NAT ở bước (1) sẽ sửa lại địa chỉ nguồn của các packet từ Node1_IP thành VIP1 và gửi các packet này trả về ExIP. Khi đó trên ExIP nhận được các packet (VIP1 -> ExIP).
Ý nghĩa của mô hình
Mô hình NAT sử dụng trong trường hợp cần cung cấp các Node dịch vụ ở trong một dải mạng nội bộ. Các Node dịch vụ được cách ly hoàn toàn với môi trường mạng bên ngoài, giúp đảm bảo an toàn.
Tiết kiệm địa chỉ IP external: toàn bộ hệ thống chỉ sử dụng 3 địa chỉ IP external. Số lượng địa chỉ IP external không phụ thuộc vào số lượng Node dịch vụ.
Dữ liệu vào và ra luôn cần đi qua node LB Active nên trong trường hợp dữ liệu trả về lớn có thể gây ảnh hưởng đến hiệu năng.
Mô hình này thường dùng trong trường hợp node dịch vụ chỉ nhận kết nối đến, không có nhu cầu chủ động kết nối ra bên ngoài.
4.2 Mô hình ROUTE
Trong mô hình ROUTE, các connection được route từ LB xuống các Node dịch vụ. Mô hình triển khai như sau (lưu ý các IP trong mô hình này chỉ là ví dụ, có thể chỉnh sửa tùy theo thực tế):
Mô hình này gồm các thành phần sau:
Mô hình Route
Hai node LB1, LB2 hoạt động theo mô hình Active/Passive cung cấp dịch vụ Virtual IP và Load Balancer. Hai node này chỉ cần một network interface (network interface cho kết nối external). LB1 và LB2 được cài đặt các phần mềm corosync, pacemaker và ldirectord. Corosync và pacemaker thực hiện HA trên hai node theo mô hình Active/Passive. Resource ldirectord chạy theo mô hình NAT. Resource Virtual IP cung cấp 1 VIP cho dải mạng external.
Các node dịch vụ chỉ cài đặt các phần mềm dịch vụ. Các node dịch vụ có một network interface thuộc dải external. Các node này có thể thêm, bớt tùy theo nhu cầu. Mỗi lần thêm bớt các node dịch vụ chỉ cần cấu hình ldirectord trên LB1 và LB2. Tất cả các node dịch vụ đều có một loopback interface ảo là lo:0 có IP là VIP.
Luồng hoạt động của mô hình ROUTE như sau:
Route Flow
Giả sử, hiện tại node LB1 đang là Active, node LB2 là Passive. Khi đó các dịch vụ Virtual IP và Ldirectord sẽ chạy trên node LB1.
(1) Từ một máy tính bên ngoài (ExIP) kết nối tới hệ thống theo IP là VIP: 192.168.1.10. Trên LB1 nhận được các packet (ExIP -> VIP).
(2) Dịch vụ Load balancer sẽ thực hiện cân bằng tải xuống các Node1, Node2, Node3. Do sử dụng mô hình ROUTE nên LB1 sẽ thực hiện chuyển tiếp nguyên vẹn gói tin xuống địa chỉ của node dịch vụ tương ứng. Giả sử, LB1 thực hiện LB xuống Node1. Khi đó trên Node1 nhận được các packet (ExIP -> VIP).
(3) Do Node1 có địa chỉ lo:0 chính là VIP nên các packet sẽ không bị drop mà được xử lý như bình thường. Node1 xử lý và gửi kết quả trả về là các packet (VIP -> ExIP). Các packet này được gửi trực tiếp về ExIP.
Mô hình ROUTE sử dụng trong trường hợp cần cung cấp các Node dịch vụ ở trong một dải mạng external. Các Node dịch vụ hoạt động trong môi trường mạng bên ngoài nên cần chú ý cấu hình tường lửa, hạn chế mở các dịch vụ để đảm bảo an toàn.
Ý nghĩa của mô hình
Tốn địa chỉ IP external: Số lượng địa chỉ IP external phụ thuộc vào số lượng Node dịch vụ.
Dữ liệu ra không đi qua node LB Active.
Mô hình này thường dùng trong trường hợp node dịch vụ có nhu cầu chủ động kết nối ra bên ngoài.
V. HƯỚNG DẪN CÀI ĐẶT HA-LB
5.1 Môi trường cài đặt
Hệ điều hành: Linux (Ubuntu, Debian, CentOS)
5.2 Các phần mềm yêu cầu
STT	Tên phần mềm	Địa chỉ download
1	corosync	Có sẵn trên repository
2	pacemaker	Có sẵn trên repository
3	ipvsadm	Có sẵn trên repository
4	ldirector	Có sẵn trên repository. Riêng đối với Centos 6.x trở lên phải download thủ công tại địa chỉ: http://rpm.pbone.net/index.php3/stat/4/idpl/23860919/dir/centos_6/com/ldirectord-3.9.5-3.1.x86_64.rpm.html
5.3 Cài đặt theo mô hình NAT
Cài đặt mạng trên LB1 và LB2
Chú ý: các IP trong tài liệu này chỉ là ví dụ. Có thể chỉnh sửa tùy theo nhu cầu thực tế
Lựa chọn các IP như sau:
Đối với VIP:
VIP1: 192.168.1.10
VIP2: 10.10.0.10
Chỉ cần chuẩn bị các IP này, quá trình cấu hình, sử dụng IP này sẽ ở bước cấu hình resource và ldirectord
Đối với LB1: Cấu hình 2 NIC với IP như sau
eth0: 192.168.1.101
eth1: 10.10.0.101
Đối với LB2: Cấu hình 2 NIC với IP như sau
eth0: 192.168.1.102
eth1: 10.10.0.102
Cài đặt hostname cho LB1 và LB2
Cần đảm bảo chắc chắn LB1 và LB2 có hostname khác nhau. Có thể đặt tên hostname cho máy bằng cách chỉnh sửa nội dung trong file /etc/hostname (đối với họ Debian). Tốt nhất nên đặt tên hostname có tính chất dễ nhớ ví dụ máy LB1 có hostname là LB1, máy LB2 có hostname là LB2. Để thay đổi này có hiệu lực cần reboot lại hệ thống.
Cài đặt các gói cần thiết trên LB1 và LB2
Đối với họ Debian (Debian, Ubuntu)
apt-get update
apt-get install corosync pacemaker ipvsadm ldirectord
Đối với họ Redhat (Centos, Fedora)
yum install corosync pacemaker ipvsadm pcs ldirectord
Riêng đối với Centos 6.x trở lên sẽ không có sẵn gói ldirectord trên Repository. Truy cập vào địa chỉ: http://rpm.pbone.net/index.php3/stat/4/idpl/23860919/dir/centos_6/com/ldirectord-3.9.5-3.1.x86_64.rpm.html . Download gói ldirectord-3.9.5-3.1.x86_64.rpm. Trên LB1 và LB2 tiến hành cài đặt bằng lệnh: yum localinstall ldirectord-3.9.5-3.1.x86_64.rpm
Enable tcp forward trên LB1 và LB2
Trên LB1 và LB2, để các gói tin được xử lý trên nhiều network interface cần bật ip_forward.
Để enable ip forward cần chỉnh sửa dòng net.ipv4.ip_forward =0 trong /etc/sysctl.conf thành:
net.ipv4.ip_forward = 1
Thay đổi này chỉ có hiệu lực sau khi reboot lại hệ thống. Nếu muốn thay đổi này có hiệu lực mà không cần reboot lại hệ thống, sử dụng lệnh sau
sysctl -p
Disable startup ldirectord trên LB1 và LB2
Disable startup ldirectord
update-rc.d ldirectord disable
/etc/init.d/ldirectord stop
Cấu hình corosync trên LB1 và LB2
Trên LB1 sinh key
corosync-keygen
Gõ ngẫu nhiên bàn phím đến khi hoàn thành.
Copy key đã sinh (file /etc/corosync/authkey) từ LB1 sang LB2
scp /etc/corosync/authkey root@10.10.0.102:/etc/corosync/
Đối với họ REDHAT (Centos, Fedora), trên LB1 và LB2 tạo file cấu hình cho corosync bằng cách copy và edit nó như sau:
cp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf
Đối với họ REDHAT (Centos, Fedora), trên LB1 và LB2 thêm service pacemaker vào cấu hình của corosync bằng cách thêm vào cuối file /etc/corosync/corosync.conf nội dung sau (nếu chưa có):
service {
name: pacemaker
ver: 0
}
Trên cả LB1 và LB2 chỉnh sửa giá trị bindnetaddr trong file cấu hình /etc/corosync/corosync.conf thành:
bindnetaddr: 10.10.0.0
Chú ý: giá trị bindnetaddr chính là dải mạng nội bộ eth1
Trên cả LB1 và LB2, phân quyền cho file authkey
chmod 400 /etc/corosync/authkey
chown root:root /etc/corosync/authkey
Trên cả LB1 và LB2, cấu hình cho corosync khởi động cùng hệ thống bằng cách sửa giá trị START = no trong file /etc/default/corosync thành:
START = yes
Chú ý: Đối với họ REDHAT, nếu không thấy file /etc/default/corosync có thể bỏ qua bước này và chuyển sang hướng cấu hình khởi động cùng hệ thống bằng cách sau:
chkconfig corosync on
chkconfig pacemaker on
Chú ý: Mặc định đối với corosync 1.x và pacemaker 1.x. Khi corosync start, nó cũng đồng thời gọi pacemaker start cùng. Tuy nhiên trên Ubuntu 14.04 trở đi sử dụng corosync 2.x và pacemaker 1.x thì corosync không hỗ trợ gọi pacemaker start khi nó hoạt động. Do vậy đối với Ubuntu 14.04 trở đi cần cấu hình cho pacemaker startup cùng hệ thống như sau:
update-rc.d corosync defaults
update-rc.d corosync enable
update-rc.d pacemaker defaults
update-rc.d pacemaker enable
Trên LB1 và LB2 tiến hành kiểm tra việc cài đặt bằng cách đồng thời reboot lại hệ thống trên cả 2 máy. Sau đó lần lượt kiểm tra bằng lệnh sau
crm_mon -1
Nếu xuất hiện online lb1, lb2 là đã cài đặt thành công.
Cấu hình ldirectord trên LB1 và LB2
Trên LB1 tạo file /etc/ldirectord.cf có nội dung tương tự như ví dụ sau:
checktimeout=3
checkinterval=2
autoreload=yes
quiescent=yes
logfile="/var/log/ldirectord.log"
virtual=192.168.1.10:5672
real=10.10.0.111:5672 masq
real=10.10.0.112:5672 masq
real=10.10.0.113:5672 masq
scheduler=wlc
checktype=connect
Trong đó:
checktimeout: thời gian timeout tính bằng giây cho việc kiểm tra connection. Ldirectord kiểm tra trạng thái của các real server, nếu sau timeout giây mà không nhận được response từ real server nó sẽ quyết định real server đã chết.
checkinterval: giá trị tính bằng giây, thời gian định kỳ kiểm tra trạng thái hoạt động của real server.
autoreload: tự động reload file cấu hình ldirectord.cf mà không cần restart ldirectord.
logfile: đường dẫn tới file log
Các cấu hình load balencer, có thể có nhiều cấu hình LB cho nhiều service khác nhau, mỗi server bao gồm một port khác nhau. Cụ thể một cấu hình LB cho một service như sau:
virtual: địa chỉ VIP1
real: các real server, trong đó tham số masq định nghĩa phương thức forwarding các gói tin theo mô hình NAT.
scheduler: giải thuật cân bằng tải.
checktype: phương thức check trạng thái real server
Chi tiết về các cấu hình của ldirectord có thể tham khảo tại: https://github.com/ClusterLabs/resource-agents/blob/master/ldirectord/ldirectord.cf và http://manpages.ubuntu.com/manpages/dapper/en/man8/ldirectord.8.html
Copy file /etc/ldirectord.cf từ LB1 sang LB2
Chú ý: mỗi khi chỉnh sửa file /etc/ldirectord.cf cần đồng bộ nội dung trên cả LB1 và LB2.
Cấu hình resource trên LB1
Do LB1 và LB2 hoạt động cluster với nhau nên các cấu hình resource được tự động đồng bộ  chỉ cần cấu hình trên LB1 hoặc LB2. Trong tài liệu này, lựa chọn cấu hình trên LB1
Tạo file /root/config.txt với nội dung sau
primitive ClusterVIP1 ocf:heartbeat:IPaddr2 params ip="192.168.1.10" cidr_netmask="24" nic="eth0" op monitor interval="5s" timeout="20s"
primitive ClusterVIP2 ocf:heartbeat:IPaddr2 params ip="10.10.0.10" cidr_netmask="24" nic="eth1" op monitor interval="5s" timeout="20s"
primitive Ldirectord ocf:heartbeat:ldirectord params configfile="/etc/ldirectord.cf" op monitor interval="5s" timeout="20s" meta target-role="Started"
group ClusterResourceGroup ClusterVIP1 ClusterVIP2 Ldirectord
property stonith-enabled="false" no-quorum-policy="ignore"
Trong đó:
Có 3 resource là VIP1, VIP2 và Ldirectord.
Các địa chỉ VIP1 và VIP2 có thể chỉnh sửa tùy theo nhu cầu thực tế
Thực hiện load nội dung file cấu hình resource bằng các lệnh sau:
crm configure
load replace /root/config.txt
commit
show
quit
Nếu thực hiện lỗi hoặc muốn chỉnh sửa cấu hình resource cần xóa toàn bộ cấu hình trước khi thực hiện load lại. Cách xóa cấu hình resource như sau:
crm configure
property stop-all-resources=true
commit
erase
commit
Lưu ý trên Centos 6.x trở đi, các phiên bản gần đây không hỗ trợ shell crm mà thay thế bằng pcs. Cách thực hiện cấu hình các resource như sau:
pcs property set stonith-enabled="false" no-quorum-policy="ignore"
pcs resource create ClusterVIP1 ocf:heartbeat:IPaddr2 params ip="192.168.1.10" cidr_netmask="24" nic="eth0" op monitor interval="5s" timeout="20s" --group ClusterResourceGroup
pcs resource create ClusterVIP2 ocf:heartbeat:IPaddr2 params ip="10.10.0.10" cidr_netmask="24" nic="eth1" op monitor interval="5s" timeout="20s" --group ClusterResourceGroup
pcs resource create Ldirectord ocf:heartbeat:ldirectord params configfile="/etc/ldirectord.cf" op monitor interval="5s" timeout="20s" meta target-role="Started" --group ClusterResourceGroup
Để xóa các resource cũ có thể thực hiện các lệnh sau:
pcs resource delete ClusterVIP1
pcs resource delete ClusterVIP2
pcs resource delete Ldirectord
Kiểm tra cài đặt:
crm_mon -1
Nếu kết quả có dạng như sau là thành công:
Online: [ Lb2 lb1 ]
Resource Group: ClusterResourceGroup
ClusterVIP1 (ocf::heartbeat:IPaddr2): Started Lb2
ClusterVIP2 (ocf::heartbeat:IPaddr2): Started Lb2
Ldirectord (ocf::heartbeat:ldirectord): Started Lb2
Kết quả này là các dịch vụ VIP1, VIP2, Ldirectord đang chạy trên LB2
Trên máy LB2 kiểm tra IP bằng lệnh
ip addr
Nếu xuất hiện global secondary eth0 và eth1 với IP là 192.168.1.10 và 10.10.0.10 thì VIP1 và VIP2 đã chạy thành công.
Để kiểm tra dịch vụ ldirectord thực thi lệnh sau:
ipvsadm
Để test khả năng HA, trên LB2 (hoặc LB1 nếu các dịch vụ VIP1, VIP2, Ldirectord đang chạy trên LB1), reboot hoặc stop service corosync.
Trên node còn lại chạy crm_mon -1 để kiểm tra các dịch vụ đang chạy trên đó.
Cấu hình các Node dịch vụ
Tất cả các node dịch vụ cần cấu hình default gateway là địa chỉ VIP2.
Mỗi khi có nhu cầu thêm mới node dịch vụ cần cấu hình VIP2 sau đó chỉnh sửa file /etc/ldirectord.cf của LB1 và LB2 để LB xuống real server mới.
5.4 Cài đặt theo mô hình ROUTE
Cài đặt mạng trên LB1 và LB2
Chú ý: các IP trong tài liệu này chỉ là ví dụ. Có thể chỉnh sửa tùy theo nhu cầu thực tế
Lựa chọn các IP như sau:
Đối với VIP:
VIP1: 192.168.1.10
Chỉ cần chuẩn bị các IP này, quá trình cấu hình, sử dụng IP này sẽ ở bước cấu hình resource và ldirectord
Đối với LB1: Cấu hình 2 NIC với IP như sau
eth0: 192.168.1.101
Đối với LB2: Cấu hình 2 NIC với IP như sau
eth0: 192.168.1.102
Cài đặt hostname cho LB1 và LB2
Cần đảm bảo chắc chắn LB1 và LB2 có hostname khác nhau. Có thể đặt tên hostname cho máy bằng cách chỉnh sửa nội dung trong file /etc/hostname (đối với họ Debian). Tốt nhất nên đặt tên hostname có tính chất dễ nhớ ví dụ máy LB1 có hostname là LB1, máy LB2 có hostname là LB2. Để thay đổi này có hiệu lực cần reboot lại hệ thống.
Cài đặt các gói cần thiết trên LB1 và LB2
Đối với họ Debian (Debian, Ubuntu)
apt-get update
apt-get install corosync pacemaker ipvsadm ldirectord
Đối với họ Redhat (Centos, Fedora)
yum install corosync pacemaker ipvsadm pcs ldirectord Riêng đối với Centos 6.x trở lên sẽ không có sẵn gói ldirectord trên Repository. Truy cập vào địa chỉ: http://rpm.pbone.net/index.php3/stat/4/idpl/23860919/dir/centos_6/com/ldirectord-3.9.5-3.1.x86_64.rpm.html . Download gói ldirectord-3.9.5-3.1.x86_64.rpm. Trên LB1 và LB2 tiến hành cài đặt bằng lệnh: yum localinstall ldirectord-3.9.5-3.1.x86_64.rpm
Enable tcp forward trên LB1 và LB2
Trên LB1 và LB2, để các gói tin được xử lý trên nhiều network interface cần bật ip_forward.
Để enable ip forward cần chỉnh sửa dòng net.ipv4.ip_forward =0 trong /etc/sysctl.conf thành:
net.ipv4.ip_forward = 1
Thay đổi này chỉ có hiệu lực sau khi reboot lại hệ thống. Nếu muốn thay đổi này có hiệu lực mà không cần reboot lại hệ thống, sử dụng lệnh sau
sysctl -p
Disable startup ldirectord trên LB1 và LB2
Disable startup ldirectord
update-rc.d ldirectord disable
/etc/init.d/ldirectord stop
Cấu hình corosync trên LB1 và LB2
Trên LB1 sinh key
corosync-keygen
Gõ ngẫu nhiên bàn phím đến khi hoàn thành.
Copy key đã sinh (file /etc/corosync/authkey) từ LB1 sang LB2
scp /etc/corosync/authkey root@10.10.0.102:/etc/corosync/
Đối với họ REDHAT (Centos, Fedora), trên LB1 và LB2 tạo file cấu hình cho corosync bằng cách copy và edit nó như sau:
cp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf
Đối với họ REDHAT (Centos, Fedora), trên LB1 và LB2 thêm service pacemaker vào cấu hình của corosync bằng cách thêm vào cuối file /etc/corosync/corosync.conf nội dung sau (nếu chưa có):
service {
name: pacemaker
ver: 0
}
Trên cả LB1 và LB2 chỉnh sửa giá trị bindnetaddr trong file cấu hình /etc/corosync/corosync.conf thành:
bindnetaddr: 192.168.1.0
Chú ý: giá trị bindnetaddr chính là dải mạng nội bộ eth1
Trên cả LB1 và LB2, phân quyền cho file authkey
chmod 400 /etc/corosync/authkey
chown root:root /etc/corosync/authkey
Trên cả LB1 và LB2, cấu hình cho corosync khởi động cùng hệ thống bằng cách sửa giá trị START = no trong file /etc/default/corosync thành:
START = yes
Chú ý: Đối với họ REDHAT, nếu không thấy file /etc/default/corosync có thể bỏ qua bước này và chuyển sang hướng cấu hình khởi động cùng hệ thống bằng cách sau:
chkconfig corosync on
chkconfig pacemaker on
Chú ý: Mặc định đối với corosync 1.x và pacemaker 1.x. Khi corosync start, nó cũng đồng thời gọi pacemaker start cùng. Tuy nhiên trên Ubuntu 14.04 trở đi sử dụng corosync 2.x và pacemaker 1.x thì corosync không hỗ trợ gọi pacemaker start khi nó hoạt động. Do vậy đối với Ubuntu 14.04 trở đi cần cấu hình cho pacemaker startup cùng hệ thống như sau:
update-rc.d corosync defaults
update-rc.d corosync enable
update-rc.d pacemaker defaults
update-rc.d pacemaker enable
Trên LB1 và LB2 tiến hành kiểm tra việc cài đặt bằng cách đồng thời reboot lại hệ thống trên cả 2 máy. Sau đó lần lượt kiểm tra bằng lệnh sau
crm_mon -1
Nếu xuất hiện online lb1, lb2 là đã cài đặt thành công.
Cấu hình ldirectord trên LB1 và LB2
Trên LB1 tạo file /etc/ldirectord.cf có nội dung tương tự như ví dụ sau:
checktimeout=3
checkinterval=2
autoreload=yes
quiescent=yes
logfile="/var/log/ldirectord.log"
virtual=192.168.1.10:5672
real=192.168.1.111:5672 gate
real=192.168.1.112:5672 gate
real=192.168.1.113:5672 gate
scheduler=wlc
checktype=connect
Trong đó:
checktimeout: thời gian timeout tính bằng giây cho việc kiểm tra connection. Ldirectord kiểm tra trạng thái của các real server, nếu sau timeout giây mà không nhận được response từ real server nó sẽ quyết định real server đã chết.
checkinterval: giá trị tính bằng giây, thời gian định kỳ kiểm tra trạng thái hoạt động của real server.
autoreload: tự động reload file cấu hình ldirectord.cf mà không cần restart ldirectord.
logfile: đường dẫn tới file log
Các cấu hình load balencer, có thể có nhiều cấu hình LB cho nhiều service khác nhau, mỗi server bao gồm một port khác nhau. Cụ thể một cấu hình LB cho một service như sau:
virtual: địa chỉ VIP1
real: các real server, trong đó tham số gate định nghĩa phương thức forwarding các gói tin theo mô hình ROUTE.
scheduler: giải thuật cân bằng tải.
checktype: phương thức check trạng thái real server
Chi tiết về các cấu hình của ldirectord có thể tham khảo tại: https://github.com/ClusterLabs/resource-agents/blob/master/ldirectord/ldirectord.cf và http://manpages.ubuntu.com/manpages/dapper/en/man8/ldirectord.8.html
Copy file /etc/ldirectord.cf từ LB1 sang LB2
Chú ý: mỗi khi chỉnh sửa file /etc/ldirectord.cf cần đồng bộ nội dung trên cả LB1 và LB2.
Cấu hình resource trên LB1
Do LB1 và LB2 hoạt động cluster với nhau nên các cấu hình resource được tự động đồng bộ  chỉ cần cấu hình trên LB1 hoặc LB2. Trong tài liệu này, lựa chọn cấu hình trên LB1
Tạo file /root/config.txt với nội dung sau
primitive ClusterVIP ocf:heartbeat:IPaddr2 params ip="192.168.1.10" cidr_netmask="24" nic="eth0" op monitor interval="5s" timeout="20s"
primitive Ldirectord ocf:heartbeat:ldirectord params configfile="/etc/ldirectord.cf" op monitor interval="5s" timeout="20s" meta target-role="Started"
group ClusterResourceGroup ClusterVIP Ldirectord
property stonith-enabled="false" no-quorum-policy="ignore"
Trong đó:
Có 2 resource là VIP và Ldirectord.
Địa chỉ VIP có thể chỉnh sửa tùy theo nhu cầu thực tế
Thực hiện load nội dung file cấu hình resource bằng các lệnh sau:
crm configure
load replace /root/config.txt
commit
show
quit
Nếu thực hiện lỗi hoặc muốn chỉnh sửa cấu hình resource cần xóa toàn bộ cấu hình trước khi thực hiện load lại. Cách xóa cấu hình resource như sau:
crm configure
property stop-all-resources=true
commit
erase
commit
Lưu ý trên Centos 6.x trở đi, các phiên bản gần đây không hỗ trợ shell crm mà thay thế bằng pcs. Cách thực hiện cấu hình các resource như sau:
pcs property set stonith-enabled="false" no-quorum-policy="ignore"
pcs resource create ClusterVIP ocf:heartbeat:IPaddr2 params ip="192.168.1.10" cidr_netmask="24" nic="eth0" op monitor interval="5s" timeout="20s" --group ClusterResourceGroup
pcs resource create Ldirectord ocf:heartbeat:ldirectord params configfile="/etc/ldirectord.cf" op monitor interval="5s" timeout="20s" meta target-role="Started" --group ClusterResourceGroup
Để xóa các resource cũ có thể thực hiện các lệnh sau:
pcs resource delete ClusterVIP
pcs resource delete Ldirectord
Kiểm tra cài đặt:
crm_mon -1
Nếu kết quả có dạng như sau là thành công:
Online: [ Lb2 lb1 ]
Resource Group: ClusterResourceGroup
ClusterVIP (ocf::heartbeat:IPaddr2): Started Lb2
Ldirectord (ocf::heartbeat:ldirectord): Started Lb2
Kết quả này là các dịch vụ VIP, Ldirectord đang chạy trên LB2
Trên máy LB2 kiểm tra IP bằng lệnh
ip addr
Nếu xuất hiện global secondary eth0 với IP là 192.168.1.10 thì VIP đã chạy thành công.
Để kiểm tra dịch vụ ldirectord thực thi lệnh sau:
ipvsadm
Để test khả năng HA, trên LB2 (hoặc LB1 nếu các dịch vụ VIP, Ldirectord đang chạy trên LB1), reboot hoặc stop service corosync.
Trên node còn lại chạy crm_mon -1 để kiểm tra các dịch vụ đang chạy trên đó.
Cấu hình các Node dịch vụ
Tất cả các node dịch vụ cần cấu hình như sau:
Cấu hình loopback ảo là địa chỉ VIP:
lo:0 192.168.1.10
netmask: 255.255.255.255
Do tất cả các node đều có IP là VIP nên để tránh vấn đề tất cả các node đều phản hồi khi nhận được bản tin ARP hỏi VIP cần disable việc trả lời arp trên các node dịch vụ bằng cách chỉnh sửa nội dung file /etc/sysctl.conf như sau:
net.ipv4.conf.all.arp_announce = 2
net.ipv4.conf.all.arp_ignore = 1
Sau đó reboot lại hệ thống để thay đổi này có hiệu lực
Mỗi khi có nhu cầu thêm mới node dịch vụ cần cấu hình như trên sau đó chỉnh sửa file /etc/ldirectord.cf của LB1 và LB2 để LB xuống real server mới.
